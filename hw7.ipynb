{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c790de75",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw7.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eca5e1-717f-451f-b23f-ef411de8576f",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 7: Word embeddings and topic modeling \n",
    "**Due date: See the [Calendar](https://htmlpreview.github.io/?https://github.com/UBC-CS/cpsc330/blob/master/docs/calendar.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f5cb55-0576-4596-ba0e-88d0ad7c39f1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4651888-484b-42a0-95e1-d273e5069205",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b799e3-3d4e-4151-9123-9d3733a63ac3",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da966a1-1d24-42a8-b959-e42202e6b824",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "## Submission instructions\n",
    "<hr>\n",
    "rubric={points}\n",
    "\n",
    "**Please be aware that this homework assignment requires installation of several packages in your course environment. It's possible that you'll encounter installation challenges, which might be frustrating. However, remember that solving these issues is not wasting time but it is an essential skill for anyone aspiring to work in data science or machine learning.**\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330-2024W1/blob/main/docs/homework_instructions.md). \n",
    "\n",
    "**You may work in a group on this homework and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 2. \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members).\n",
    "\n",
    "\n",
    "When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from “1” will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission.\n",
    "4. Make sure that the plots and output are rendered properly in your submitted file. \n",
    "5. If the .ipynb file is too big and doesn't render on Gradescope, also upload a pdf or html in addition to the .ipynb. If the pdf or html also fail to render on Gradescope, please create two files for your homework: hw6a.ipynb with Exercise 1 and hw6b.ipynb with Exercises 2 and 3 and submit these two files in your submission.  \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be5b2d-1854-4c63-bcc6-9b6258b7293a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859b3f00-a3e5-45d8-b504-22a84ace38cd",
   "metadata": {},
   "source": [
    "## Exercise 1:  Exploring pre-trained word embeddings <a name=\"1\"></a>\n",
    "<hr>\n",
    "\n",
    "In lecture 18, we talked about natural language processing (NLP). Using pre-trained word embeddings is very common in NLP. It has been shown that pre-trained word embeddings work well on a variety of text classification tasks. These embeddings are created by training a model like Word2Vec on a huge corpus of text such as a dump of Wikipedia or a dump of the web crawl. \n",
    "\n",
    "A number of pre-trained word embeddings are available out there. Some popular ones are: \n",
    "\n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "    * trained using [the GloVe algorithm](https://nlp.stanford.edu/pubs/glove.pdf) \n",
    "    * published by Stanford University \n",
    "- [fastText pre-trained embeddings for 294 languages](https://fasttext.cc/docs/en/pretrained-vectors.html) \n",
    "    * trained using the fastText algorithm\n",
    "    * published by Facebook\n",
    "    \n",
    "In this exercise, you will be exploring GloVe Wikipedia pre-trained embeddings. The code below loads the word vectors trained on Wikipedia using an algorithm called Glove. You'll need `gensim` package in your cpsc330 conda environment to run the code below. \n",
    "\n",
    "```\n",
    "> conda activate cpsc330\n",
    "> conda install -c anaconda gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4823523-ca44-48a3-94bb-f6e453d27f1c",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    "\n",
    "print(list(gensim.downloader.info()[\"models\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83e4717e-215b-4c1b-b08a-9f5adbb52467",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# This will take a while to run when you run it for the first time.\n",
    "import gensim.downloader as api\n",
    "\n",
    "glove_wiki_vectors = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76ec38c4-ce89-4372-b015-035f4d682132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_wiki_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c78dafb-f712-447a-b870-1fac6c249e5f",
   "metadata": {},
   "source": [
    "There are 400,000 word vectors in this pre-trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea1002-2451-4008-aa83-54618c757bb3",
   "metadata": {},
   "source": [
    "Now that we have GloVe Wiki vectors loaded in `glove_wiki_vectors`, let's explore the embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a75ac-fd18-4a53-89d3-26f1051c4ef3",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8119fb78-d2be-4ccf-8c8d-31026563e072",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.1 Word similarity using pre-trained embeddings\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- Come up with a list of 4 words of your choice and find similar words to these words using `glove_wiki_vectors` embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787fe4d0-4206-4747-9638-7782cca51d3f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6205ebad-49a9-435a-bf84-ae57d29c2068",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_words = ['dancer', 'woman', 'data', 'english']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d59251b0-12c0-41cf-867b-3e5cb90f4d8d",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similar words to dancer are:\n",
      "[('choreographer', 0.7776089310646057), ('ballerina', 0.7597374320030212), ('performer', 0.7322809100151062), ('dancers', 0.718708336353302), ('ballet', 0.7059119939804077), ('actress', 0.6762431263923645), ('dance', 0.6759555339813232), ('singer', 0.6707386374473572), ('musician', 0.6590761542320251), ('pianist', 0.6540020108222961)]\n",
      "The similar words to woman are:\n",
      "[('girl', 0.8472672700881958), ('man', 0.8323494791984558), ('mother', 0.827568769454956), ('boy', 0.7720510363578796), ('she', 0.7632068395614624), ('child', 0.7601761817932129), ('wife', 0.7505022883415222), ('her', 0.7445706129074097), ('herself', 0.7426273822784424), ('daughter', 0.7264456152915955)]\n",
      "The similar words to data are:\n",
      "[('information', 0.7920401096343994), ('analysis', 0.7564454674720764), ('tracking', 0.7226751446723938), ('database', 0.7215273976325989), ('system', 0.6736716032028198), ('computer', 0.6733997464179993), ('statistics', 0.6705881953239441), ('systems', 0.6695857048034668), ('applications', 0.6660565733909607), ('numbers', 0.6655444502830505)]\n",
      "The similar words to english are:\n",
      "[('welsh', 0.752077043056488), ('language', 0.7214903831481934), ('irish', 0.7088090777397156), ('scottish', 0.6993110179901123), ('british', 0.6677687764167786), ('translation', 0.6405038237571716), ('word', 0.6380666494369507), ('arabic', 0.6370865702629089), ('french', 0.63337242603302), ('languages', 0.6292626857757568)]\n"
     ]
    }
   ],
   "source": [
    "for word in list_words:\n",
    "    print(f'The similar words to {word} are:\\n{glove_wiki_vectors.most_similar(word)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24714da4-3ee5-424e-9a21-bd4b33de2e08",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6461c1d5-42ff-4267-bf04-e3642c5b40bb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.2 Word similarity using pre-trained embeddings\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Calculate cosine similarity for the following word pairs (`word_pairs`) using the [`similarity`](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=similarity#gensim.models.keyedvectors.KeyedVectors.similarity) method of `glove_wiki_vectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b37d06f-81c7-4120-8dc7-2270105d5ecb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "word_pairs = [\n",
    "    (\"coast\", \"shore\"),\n",
    "    (\"clothes\", \"closet\"),\n",
    "    (\"old\", \"new\"),\n",
    "    (\"smart\", \"intelligent\"),\n",
    "    (\"dog\", \"cat\"),\n",
    "    (\"tree\", \"lawyer\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77427682-97e3-4fa2-b2d6-84940fce9e27",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7c8c6c4-7113-4cb2-98f8-d34ec1c632e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity between coast and shore is: 0.7000272274017334\n",
      "The cosine similarity between clothes and closet is: 0.5462759733200073\n",
      "The cosine similarity between old and new is: 0.6432487964630127\n",
      "The cosine similarity between smart and intelligent is: 0.7552732229232788\n",
      "The cosine similarity between dog and cat is: 0.8798074722290039\n",
      "The cosine similarity between tree and lawyer is: 0.07671945542097092\n"
     ]
    }
   ],
   "source": [
    "for pairs in word_pairs:\n",
    "    print(f'The cosine similarity between {pairs[0]} and {pairs[1]} is: {glove_wiki_vectors.similarity(pairs[0], pairs[1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1a6420-ae43-4469-99e1-59a966238a43",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d9186e-ab44-43e5-8a96-7a1c4130b598",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.3 Representation of all words in English\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. The vocabulary size of Wikipedia embeddings is quite large. The `test_words` list below contains a few new words (called neologisms) and biomedical domain-specific abbreviations. Write code to check whether `glove_wiki_vectors` have representation for these words or not. \n",
    "> If a given word `word` is in the vocabulary, `word in glove_wiki_vectors` will return True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be6a6488-5139-43cb-a88c-b1d45df700cf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "test_words = [\n",
    "    \"covididiot\",\n",
    "    \"fomo\",\n",
    "    \"frenemies\",\n",
    "    \"anthropause\",\n",
    "    \"photobomb\",\n",
    "    \"selfie\",\n",
    "    \"pxg\",  # Abbreviation for pseudoexfoliative glaucoma\n",
    "    \"pacg\",  # Abbreviation for primary angle closure glaucoma\n",
    "    \"cct\",  # Abbreviation for central corneal thickness\n",
    "    \"escc\",  # Abbreviation for esophageal squamous cell carcinoma\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfef3be-698f-4792-b7c3-46fac945e7f5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44097c5d-97a0-450e-af88-73c9a4c90c94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def in_wiki_embeds(word):\n",
    "    '''\n",
    "    returns True if given word is in Wikipedia embeddings vocabulary, False if not\n",
    "    '''\n",
    "    return word in glove_wiki_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28e898fa-94ab-423b-8117-a9157f6d3364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word covididiot is not in the Wikipedia embeddings.\n",
      "The word fomo is not in the Wikipedia embeddings.\n",
      "The word frenemies is in the Wikipedia embeddings.\n",
      "The word anthropause is not in the Wikipedia embeddings.\n",
      "The word photobomb is not in the Wikipedia embeddings.\n",
      "The word selfie is not in the Wikipedia embeddings.\n",
      "The word pxg is not in the Wikipedia embeddings.\n",
      "The word pacg is not in the Wikipedia embeddings.\n",
      "The word cct is in the Wikipedia embeddings.\n",
      "The word escc is in the Wikipedia embeddings.\n"
     ]
    }
   ],
   "source": [
    "for word in test_words:\n",
    "    if in_wiki_embeds(word):\n",
    "        print(f'The word {word} is in the Wikipedia embeddings.')\n",
    "    else:\n",
    "        print(f'The word {word} is not in the Wikipedia embeddings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2155dbe-979f-4e4d-bd4c-04d2a5f0be20",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ea8f10-8bfb-4698-b76c-60f5f8256d5d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.4 Stereotypes and biases in embeddings\n",
    "rubric={points}\n",
    "\n",
    "Word vectors contain lots of useful information. But they also contain stereotypes and biases of the texts they were trained on. In the lecture, we saw an example of gender bias in Google News word embeddings. Here we are using pre-trained embeddings trained on Wikipedia data. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Explore whether there are any worrisome biases or stereotypes present in these embeddings by trying out at least 4 examples. You can use the following two methods or other methods of your choice to explore this. \n",
    "    - the `analogy` function below which gives word analogies (an example shown below)\n",
    "    - [similarity](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=similarity#gensim.models.keyedvectors.KeyedVectors.similarity) or [distance](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=distance#gensim.models.keyedvectors.KeyedVectors.distances) methods (an example is shown below)\n",
    "\n",
    "> Note that most of the recent embeddings are de-biased. But you might still observe some biases in them. Also, not all stereotypes present in pre-trained embeddings are necessarily bad. But you should be aware of them when you use them in your models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a90c0cd-7cf2-4f82-a617-9a87c526281d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def analogy(word1, word2, word3, model=glove_wiki_vectors):\n",
    "    \"\"\"\n",
    "    Returns analogy word using the given model.\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    word1 : (str)\n",
    "        word1 in the analogy relation\n",
    "    word2 : (str)\n",
    "        word2 in the analogy relation\n",
    "    word3 : (str)\n",
    "        word3 in the analogy relation\n",
    "    model :\n",
    "        word embedding model\n",
    "\n",
    "    Returns\n",
    "    ---------------\n",
    "        pd.dataframe\n",
    "    \"\"\"\n",
    "    print(\"%s : %s :: %s : ?\" % (word1, word2, word3))\n",
    "    sim_words = model.most_similar(positive=[word3, word2], negative=[word1])\n",
    "    return pd.DataFrame(sim_words, columns=[\"Analogy word\", \"Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaeec14-64c1-4226-b60f-849489077ddd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Examples of using analogy to explore biases and stereotypes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e914a9fa-e1e9-4182-a82d-41737020e44f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : doctor :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nurse</td>\n",
       "      <td>0.773523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>physician</td>\n",
       "      <td>0.718943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doctors</td>\n",
       "      <td>0.682433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>patient</td>\n",
       "      <td>0.675068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dentist</td>\n",
       "      <td>0.672603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pregnant</td>\n",
       "      <td>0.664246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>medical</td>\n",
       "      <td>0.652045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nursing</td>\n",
       "      <td>0.645348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mother</td>\n",
       "      <td>0.639333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hospital</td>\n",
       "      <td>0.638750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Analogy word     Score\n",
       "0        nurse  0.773523\n",
       "1    physician  0.718943\n",
       "2      doctors  0.682433\n",
       "3      patient  0.675068\n",
       "4      dentist  0.672603\n",
       "5     pregnant  0.664246\n",
       "6      medical  0.652045\n",
       "7      nursing  0.645348\n",
       "8       mother  0.639333\n",
       "9     hospital  0.638750"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"man\", \"doctor\", \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fceb0128-3c9b-4674-ae40-da3d80f3f00c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14283238"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"aboriginal\", \"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "774cb08e-95bc-458b-bc78-ee3fcf2b2f7a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35182396"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"white\", \"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55bef6d-9a57-41a4-8a83-a65d7bc07783",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a227771e-f8b7-4bad-9881-eb5528f10e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman : caretaker :: man : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>interim</td>\n",
       "      <td>0.582048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boss</td>\n",
       "      <td>0.574412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>manager</td>\n",
       "      <td>0.534280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>appointed</td>\n",
       "      <td>0.528382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>prime</td>\n",
       "      <td>0.523247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>premier</td>\n",
       "      <td>0.519685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sacked</td>\n",
       "      <td>0.518245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pleat</td>\n",
       "      <td>0.507855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>venables</td>\n",
       "      <td>0.501967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>successor</td>\n",
       "      <td>0.499100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Analogy word     Score\n",
       "0      interim  0.582048\n",
       "1         boss  0.574412\n",
       "2      manager  0.534280\n",
       "3    appointed  0.528382\n",
       "4        prime  0.523247\n",
       "5      premier  0.519685\n",
       "6       sacked  0.518245\n",
       "7        pleat  0.507855\n",
       "8     venables  0.501967\n",
       "9    successor  0.499100"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"woman\", \"caretaker\", \"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "575a61e2-5e29-4fae-9f8a-13c2a9407132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scientist : man :: homemaker : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>housewife</td>\n",
       "      <td>0.690942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mother</td>\n",
       "      <td>0.589116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom</td>\n",
       "      <td>0.583259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>woman</td>\n",
       "      <td>0.581701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>girl</td>\n",
       "      <td>0.578157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>grandmother</td>\n",
       "      <td>0.568886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>schoolteacher</td>\n",
       "      <td>0.566959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>waitress</td>\n",
       "      <td>0.553455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>shopkeeper</td>\n",
       "      <td>0.547613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>girlfriend</td>\n",
       "      <td>0.545629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Analogy word     Score\n",
       "0      housewife  0.690942\n",
       "1         mother  0.589116\n",
       "2            mom  0.583259\n",
       "3          woman  0.581701\n",
       "4           girl  0.578157\n",
       "5    grandmother  0.568886\n",
       "6  schoolteacher  0.566959\n",
       "7       waitress  0.553455\n",
       "8     shopkeeper  0.547613\n",
       "9     girlfriend  0.545629"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"scientist\", \"man\", \"homemaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "deaa74e9-858e-4b28-a7a8-6b6b522c1ef6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3121177"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"blonde\", \"smart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "93cf446e-d0c0-4c1c-af65-8ff55637eed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4982603"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"blonde\", \"dumb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8493ff3a-9bd9-4b72-8ff7-f939c105e88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "server : boss :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>man</td>\n",
       "      <td>0.730424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>girl</td>\n",
       "      <td>0.640664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>father</td>\n",
       "      <td>0.635410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boy</td>\n",
       "      <td>0.634364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>husband</td>\n",
       "      <td>0.616292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wife</td>\n",
       "      <td>0.615824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>brother</td>\n",
       "      <td>0.611986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>friend</td>\n",
       "      <td>0.610112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>boyfriend</td>\n",
       "      <td>0.602780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>son</td>\n",
       "      <td>0.600712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Analogy word     Score\n",
       "0          man  0.730424\n",
       "1         girl  0.640664\n",
       "2       father  0.635410\n",
       "3          boy  0.634364\n",
       "4      husband  0.616292\n",
       "5         wife  0.615824\n",
       "6      brother  0.611986\n",
       "7       friend  0.610112\n",
       "8    boyfriend  0.602780\n",
       "9          son  0.600712"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"server\", \"boss\", \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cad9e4e5-8b44-4b64-8c5e-ebd344c8fd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boy : strong :: girl : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stronger</td>\n",
       "      <td>0.757472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>despite</td>\n",
       "      <td>0.698031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>strongest</td>\n",
       "      <td>0.697400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weak</td>\n",
       "      <td>0.680318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>solid</td>\n",
       "      <td>0.677007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>very</td>\n",
       "      <td>0.676151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>robust</td>\n",
       "      <td>0.671619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>particularly</td>\n",
       "      <td>0.667278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>consistent</td>\n",
       "      <td>0.662579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>good</td>\n",
       "      <td>0.657380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Analogy word     Score\n",
       "0      stronger  0.757472\n",
       "1       despite  0.698031\n",
       "2     strongest  0.697400\n",
       "3          weak  0.680318\n",
       "4         solid  0.677007\n",
       "5          very  0.676151\n",
       "6        robust  0.671619\n",
       "7  particularly  0.667278\n",
       "8    consistent  0.662579\n",
       "9          good  0.657380"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"boy\", \"strong\", \"girl\") # a de-biased example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f04b87-5fa0-4eb4-bb50-cb21aa7ffd1c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f474b6ac-25f3-45f1-97db-bfadf71d9f80",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.5 Discussion\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Discuss your observations from 1.4. Are there any worrisome biases in these embeddings trained on Wikipedia?   \n",
    "2. Give an example of how using embeddings with biases could cause harm in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e95ea8-80c0-4184-8b89-1fa1581404f1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_5\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c125c3",
   "metadata": {},
   "source": [
    "It seems that gender bias is still present in the Wikipedia embeddings, depsite some trials that appeared to be de-biased. Classical stereotypes such as 'dumb blondes' also seem to peek through in these pre-trained embeddings. Biased embeddings like these can be incredibly harmful, as they perpetuate stereotypes, regardless of whether or not these stereotypes are valid or accurately reflect society. In most cases, stereotypes like these stand to harm more than they help. Biased embeddings in particular can be incredibly harmful if, for example, a news website were to use topic modelling to cluster articles based on their contents and clustered articles based solely on stereotypes rather than objective categories. This could be highly offensive to certain groups. It is important to be aware of these biases and recognize their limitations on a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a6ab6-62ef-4fdd-ba0d-5e7e920154a3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cb13f7-e08c-4f8d-86c9-b1602cc8a5b4",
   "metadata": {},
   "source": [
    "## Exercise 2: Topic modeling \n",
    "\n",
    "The goal of topic modeling is discovering high-level themes in a large collection of texts. \n",
    "\n",
    "In this homework, you will explore topics in [the 20 newsgroups text dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html) using `scikit-learn`'s `LatentDirichletAllocation` (LDA) model. \n",
    "\n",
    "Usually, topic modeling is used for discovering abstract \"topics\" that occur in a collection of documents when you do not know the actual topics present in the documents. But 20 newsgroups text dataset is labeled with categories (e.g., sports, hardware, religion), and you will be able to cross-check the topics discovered by your model with these available topics. \n",
    "\n",
    "The starter code below loads the train and test portion of the data and convert the train portion into a pandas DataFrame. For speed, we will only consider documents with the following 8 categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "327ca91e-cf57-4481-b4cc-46c29a9849a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9afa3b30-7f32-48ec-8291-69c964a38c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You know, I was reading 18 U.S.C. 922 and some...</td>\n",
       "      <td>6</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\nIt's not a bad question: I don't have an...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nActuallay I don't, but on the other hand I d...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The following problem is really bugging me,\\na...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n  This is the latest from UPI \\n\\n     For...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4558</th>\n",
       "      <td>Hi Everyone ::\\n\\nI am  looking for  some soft...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4559</th>\n",
       "      <td>Archive-name: x-faq/part3\\nLast-modified: 1993...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4560</th>\n",
       "      <td>\\nThat's nice, but it doesn't answer the quest...</td>\n",
       "      <td>6</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4561</th>\n",
       "      <td>Hi,\\n     I just got myself a Gateway 4DX-33V ...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4562</th>\n",
       "      <td>\\n\\n[h] \\tThe Armenians in Nagarno-Karabagh ar...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4563 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  \\\n",
       "0     You know, I was reading 18 U.S.C. 922 and some...       6   \n",
       "1     \\n\\n\\nIt's not a bad question: I don't have an...       1   \n",
       "2     \\nActuallay I don't, but on the other hand I d...       1   \n",
       "3     The following problem is really bugging me,\\na...       2   \n",
       "4     \\n\\n  This is the latest from UPI \\n\\n     For...       7   \n",
       "...                                                 ...     ...   \n",
       "4558  Hi Everyone ::\\n\\nI am  looking for  some soft...       1   \n",
       "4559  Archive-name: x-faq/part3\\nLast-modified: 1993...       2   \n",
       "4560  \\nThat's nice, but it doesn't answer the quest...       6   \n",
       "4561  Hi,\\n     I just got myself a Gateway 4DX-33V ...       2   \n",
       "4562  \\n\\n[h] \\tThe Armenians in Nagarno-Karabagh ar...       7   \n",
       "\n",
       "                target_name  \n",
       "0        talk.politics.guns  \n",
       "1             comp.graphics  \n",
       "2             comp.graphics  \n",
       "3            comp.windows.x  \n",
       "4     talk.politics.mideast  \n",
       "...                     ...  \n",
       "4558          comp.graphics  \n",
       "4559         comp.windows.x  \n",
       "4560     talk.politics.guns  \n",
       "4561         comp.windows.x  \n",
       "4562  talk.politics.mideast  \n",
       "\n",
       "[4563 rows x 3 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats = [\n",
    "    \"rec.sport.hockey\",\n",
    "    \"rec.sport.baseball\",\n",
    "    \"soc.religion.christian\",\n",
    "    \"alt.atheism\",\n",
    "    \"comp.graphics\",\n",
    "    \"comp.windows.x\",\n",
    "    \"talk.politics.mideast\",\n",
    "    \"talk.politics.guns\",\n",
    "]  # We'll only consider these categories out of 20 categories for speed.\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset=\"train\", remove=(\"headers\", \"footers\", \"quotes\"), categories=cats\n",
    ")\n",
    "X_news_train, y_news_train = newsgroups_train.data, newsgroups_train.target\n",
    "df = pd.DataFrame(X_news_train, columns=[\"text\"])\n",
    "df[\"target\"] = y_news_train\n",
    "df[\"target_name\"] = [\n",
    "    newsgroups_train.target_names[target] for target in newsgroups_train.target\n",
    "]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8095c853-d4d5-49a3-bda0-129ffd2f690b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.windows.x',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "447e9cc5-5d39-413d-97e8-bf9124241e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "      <th>text_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You know, I was reading 18 U.S.C. 922 and some...</td>\n",
       "      <td>6</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>know read u.s.c. 922 sence wonder help u.s.c. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\nIt's not a bad question: I don't have an...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>bad question ref list algorithm think bit hard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nActuallay I don't, but on the other hand I d...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>actuallay hand support idea have newsgroup asp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The following problem is really bugging me,\\na...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>follow problem bug appreciate help create wind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n  This is the latest from UPI \\n\\n     For...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>late upi foreign ministry spokesman ferhat ata...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  You know, I was reading 18 U.S.C. 922 and some...       6   \n",
       "1  \\n\\n\\nIt's not a bad question: I don't have an...       1   \n",
       "2  \\nActuallay I don't, but on the other hand I d...       1   \n",
       "3  The following problem is really bugging me,\\na...       2   \n",
       "4  \\n\\n  This is the latest from UPI \\n\\n     For...       7   \n",
       "\n",
       "             target_name                                            text_pp  \n",
       "0     talk.politics.guns  know read u.s.c. 922 sence wonder help u.s.c. ...  \n",
       "1          comp.graphics  bad question ref list algorithm think bit hard...  \n",
       "2          comp.graphics  actuallay hand support idea have newsgroup asp...  \n",
       "3         comp.windows.x  follow problem bug appreciate help create wind...  \n",
       "4  talk.politics.mideast  late upi foreign ministry spokesman ferhat ata...  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = df[:50]\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9cdcfc-7545-4a40-a9b3-34b58bb38365",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e21b066-8de6-42a7-8e4e-097fd8727367",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.1 Preprocessing using [spaCy](https://spacy.io/)\n",
    "rubric={points}\n",
    "\n",
    "Preprocessing is a crucial step before carrying out topic modeling and it markedly affects topic modeling results. In this exercise, you'll prepare the data using [spaCy](https://spacy.io/) for topic modeling. \n",
    "\n",
    "**Your tasks:** \n",
    "\n",
    "- Write code using [spaCy](https://spacy.io/) to preprocess the `text` column in the given dataframe `df` and save the processed text in a new column called `text_pp` within the same dataframe.\n",
    "\n",
    "If you do not have spaCy in your course environment, you'll have to [install it](https://spacy.io/usage) and download the pretrained model en_core_web_md. \n",
    "\n",
    "`python -m spacy download en_core_web_md`\n",
    "\n",
    "\n",
    "Note that there is no such thing as \"perfect\" preprocessing. You'll have to make your own judgments and decisions on which tokens are likely to be more informative for the given task. Some common text preprocessing steps for topic modeling include: \n",
    "- getting rid of slashes, new-line characters, or any other non-informative characters\n",
    "- sentence segmentation and tokenization      \n",
    "- replacing urls, email addresses, or numbers with generic tokens such as \"URL\",  \"EMAIL\", \"NUM\". \n",
    "- getting rid of other fairly unique tokens which are not going to help us in topic modeling  \n",
    "- excluding stopwords and punctuation \n",
    "- lemmatization\n",
    "\n",
    "\n",
    "> Check out [these available attributes](https://spacy.io/api/token#attributes) for `token` in spaCy which might help you with preprocessing. \n",
    "\n",
    "> You can also get rid of words with specific POS tags. [Here](https://universaldependencies.org/u/pos/) is the list of part-of-speech tags used in spaCy. \n",
    "\n",
    "> You may have to use regex to clean text before passing it to spaCy. Also, you might have to go back and forth between preprocessing in this exercise and and topic modeling in Exercise 2 before finalizing preprocessing steps. \n",
    "\n",
    "> Note that preprocessing the corpus might take some time. So here are a couple of suggestions: 1) During the debugging phase, work on a smaller subset of the data. 2) Once you finalize the preprocessing part, you might want to save the preprocessed data in a CSV and work with this CSV so that you don't run the preprocessing part every time you run the notebook. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6816e7a3-d6d4-4bef-81f4-9b4fdf638175",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0036dbe6-2353-4c5a-80bf-2b884a170a29",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8be7c802-d10b-4929-9664-34274299e884",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# following code adapted from Lecture 18 notes\n",
    "\n",
    "def preprocess(\n",
    "    doc,\n",
    "    min_token_len=2,\n",
    "    irrelevant_pos=[\"ADV\", \"PRON\", \"CCONJ\", \"PUNCT\", \"PART\", \"DET\", \"ADP\", \"SPACE\", \"NUM\", \"EMAIL\", \"URL\"],\n",
    "):\n",
    "    \"\"\"\n",
    "    Given text, min_token_len, and irrelevant_pos carry out preprocessing of the text\n",
    "    and return a preprocessed string.\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    doc : (spaCy doc object)\n",
    "        the spacy doc object of the text\n",
    "    min_token_len : (int)\n",
    "        min_token_length required\n",
    "    irrelevant_pos : (list)\n",
    "        a list of irrelevant pos tags\n",
    "\n",
    "    Returns\n",
    "    -------------\n",
    "    (str) the preprocessed text\n",
    "    \"\"\"\n",
    "\n",
    "    clean_text = []\n",
    "\n",
    "    for token in doc:\n",
    "        if (\n",
    "            token.is_stop == False  # Check if it's not a stopword\n",
    "            and len(token) > min_token_len  # Check if the word meets minimum threshold\n",
    "            and token.pos_ not in irrelevant_pos\n",
    "        ):  # Check if the POS is in the acceptable POS tags\n",
    "            lemma = token.lemma_  # Take the lemma of the word\n",
    "            clean_text.append(lemma.lower())\n",
    "    return \" \".join(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ba75d96d-01dc-4b3b-b222-67c63b5fd0c5",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"text_pp\"] = [preprocess(text) for text in nlp.pipe(df[\"text\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0782a094-bd30-4e80-9678-8f1197ca6a28",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gx/0f3c0wm11cs4n_87_vsk0fhw0000gn/T/ipykernel_9541/2898573282.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[\"text_pp\"] = [preprocess(text) for text in nlp.pipe(test_df[\"text\"])]\n"
     ]
    }
   ],
   "source": [
    "test_df[\"text_pp\"] = [preprocess(text) for text in nlp.pipe(test_df[\"text\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3c2b633b-6601-4b83-8668-800a99d3ba20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "      <th>text_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nActuallay I don't, but on the other hand I d...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>actuallay hand support idea have newsgroup asp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The following problem is really bugging me,\\na...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>follow problem bug appreciate help create wind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n  This is the latest from UPI \\n\\n     For...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>late upi foreign ministry spokesman ferhat ata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hi,\\n   I'd like to subscribe to Leadership Ma...</td>\n",
       "      <td>5</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>like subscribe leadership magazine wonder disk...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "2  \\nActuallay I don't, but on the other hand I d...       1   \n",
       "3  The following problem is really bugging me,\\na...       2   \n",
       "4  \\n\\n  This is the latest from UPI \\n\\n     For...       7   \n",
       "5  Hi,\\n   I'd like to subscribe to Leadership Ma...       5   \n",
       "\n",
       "              target_name                                            text_pp  \n",
       "2           comp.graphics  actuallay hand support idea have newsgroup asp...  \n",
       "3          comp.windows.x  follow problem bug appreciate help create wind...  \n",
       "4   talk.politics.mideast  late upi foreign ministry spokesman ferhat ata...  \n",
       "5  soc.religion.christian  like subscribe leadership magazine wonder disk...  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bbad26-84c1-41ed-8201-0843924b3166",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980f9bd5-31ef-43dd-9ec7-e0e1cd9a9714",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.2 Justification\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- Outline the preprocessing steps you carried out in the previous exercise (bullet point format is fine), providing a brief justification when necessary. \n",
    "\n",
    "> You might want to wait to answer this question till you are done with Exercise 2 and you have finalized the preprocessing steps in 2.1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8df885-0fb7-4ec7-bef6-bb0c3cd82e9e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a06bf7",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c22f0e-2526-4f83-94ee-a625c7a5ed04",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f080f91c-6801-4b89-9fbc-1b574b09915c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.3 Build a topic model using sklearn's LatentDirichletAllocation\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Build LDA models on the preprocessed data using using [sklearn's `LatentDirichletAllocation`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) and random state 42. Experiment with a few values for the number of topics (`n_components`). Pick a reasonable number for the number of topics and briefly justify your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e472711-3c92-4b82-a31d-5d4bf55dbf2a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb5a649",
   "metadata": {},
   "source": [
    "Because we know the true number of topics here is 8, I decided to choose a range including 8. However, I believe the topics that were included in the subset of topics in our sample can be reduced to 4: sports, religion, technology, and politics. As such, I decided to pick a range from 4-8 increasing by one using ``numpy's arange``. I believe this range will capture at least one meaningful topic segmentation. Below I perform topic modelling 5 times using ``LatentDirichletAllocation`` and ``mglearn``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "14344537-abdf-451e-ba2d-b3e44aa2bc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# following code adapted from Lecture 18\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(stop_words='english', strip_accents='ascii')\n",
    "X = vec.fit_transform(df[\"text_pp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "88908be8-85df-40bf-9d6d-0b48948a7137",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# following code adapted from Lecture 18\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import mglearn\n",
    "\n",
    "\n",
    "def model_topics(n_topics: int, X=X):\n",
    "    '''\n",
    "    takes a number of topics and performs topic modelling on test dataset containing preprocessed newsgroup text with that number of topics\n",
    "    '''\n",
    "    lda = LatentDirichletAllocation(n_components = n_topics,\n",
    "                                    learning_method=\"batch\", \n",
    "                                    max_iter=10, \n",
    "                                    random_state=42,\n",
    "                                    n_jobs=-1)\n",
    "\n",
    "    # fit topic modelling transformer to pre-processed count vectorized text data\n",
    "    news_topics = lda.fit_transform(X)\n",
    "\n",
    "    sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "    feature_names = np.array(vec.get_feature_names_out()) \n",
    "    \n",
    "    mglearn.tools.print_topics(\n",
    "    topics=range(n_topics),\n",
    "    feature_names=feature_names,\n",
    "    sorting=sorting,\n",
    "    topics_per_chunk=10,\n",
    "    n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bed16e6f-2706-463e-80cc-c64d2b75a296",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 6 7 8]\n",
      "topic 0       topic 1       topic 2       topic 3       \n",
      "--------      --------      --------      --------      \n",
      "people        file          god           game          \n",
      "say           program       people        team          \n",
      "know          use           think         year          \n",
      "gun           window        know          play          \n",
      "armenian      edu           believe       good          \n",
      "time          server        jesus         player        \n",
      "come          image         say           win           \n",
      "kill          entry         thing         season        \n",
      "think         widget        question      think         \n",
      "armenians     display       time          hockey        \n",
      "\n",
      "\n",
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "people        god           god           game          file          \n",
      "say           jesus         believe       team          image         \n",
      "know          israel        exist         year          use           \n",
      "gun           entry         atheist       play          program       \n",
      "think         know          think         player        window        \n",
      "time          church        argument      good          edu           \n",
      "come          people        question      win           server        \n",
      "armenian      file          belief        season        available     \n",
      "right         believe       point         think         run           \n",
      "kill          bible         thing         hockey        widget        \n",
      "\n",
      "\n",
      "topic 0       topic 1       topic 2       topic 3       topic 4       topic 5       \n",
      "--------      --------      --------      --------      --------      --------      \n",
      "people        jesus         god           game          xterm         file          \n",
      "say           god           period        team          god           use           \n",
      "know          output        exist         year          problem       image         \n",
      "think         church        truth         play          work          program       \n",
      "time          file          believe       good          com           window        \n",
      "come          christ        argument      player        edu           edu           \n",
      "god           sin           power         season        window        available     \n",
      "gun           lord          question      win           pt            server        \n",
      "right         write         true          think         spirit        run           \n",
      "thing         oname         faith         hockey        den           display       \n",
      "\n",
      "\n",
      "topic 0       topic 1       topic 2       topic 3       topic 4       topic 5       topic 6       \n",
      "--------      --------      --------      --------      --------      --------      --------      \n",
      "people        jesus         god           game          window        file          bos           \n",
      "say           output        believe       team          motif         use           det           \n",
      "know          file          think         year          xterm         program       chi           \n",
      "gun           christ        know          play          include       image         tor           \n",
      "think         church        thing         player        key           edu           pit           \n",
      "come          father        people        good          server        window        stl           \n",
      "time          oname         question      season        edu           display       cal           \n",
      "armenian      write         exist         win           com           run           van           \n",
      "kill          lord          good          think         available     available     que           \n",
      "right         entry         bible         hockey        run           version       buf           \n",
      "\n",
      "\n",
      "topic 0       topic 1       topic 2       topic 3       topic 4       topic 5       topic 6       topic 7       \n",
      "--------      --------      --------      --------      --------      --------      --------      --------      \n",
      "people        jesus         militia       game          xterm         file          john          god           \n",
      "say           israel        greek         team          edu           use           com           think         \n",
      "gun           god           time          year          key           program       utica         believe       \n",
      "know          israeli       state         play          server        image         baerga        people        \n",
      "armenian      output        weaver        player        include       window        adirondack    know          \n",
      "think         know          won           good          x11           edu           rochester     thing         \n",
      "come          say           right         win           font          display       baltimore     question      \n",
      "time          christ        fallacy       season        export        available     moncton       religion      \n",
      "kill          file          marriage      think         window        run           wip           church        \n",
      "armenians     jews          lost          hockey        subject       widget        binghamton    way           \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics = np.arange(4, 9, 1)\n",
    "\n",
    "print(topics)\n",
    "\n",
    "for topic in topics:\n",
    "    model_topics(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1afbdb-b419-4d17-a7fa-0dfd9e618f15",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4ff43-188c-4d9a-8404-691cd563dd9a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.4 Exploring word topic association\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "1. For the number of topics you picked in the previous exercise, show top 10 words for each of your topics and suggest labels for each of the topics (similar to how we came up with labels \"health and nutrition\", \"fashion\", and \"machine learning\" in the toy example we saw in class). \n",
    "\n",
    "> If your topics do not make much sense, you might have to go back to preprocessing in Exercise 2.1, improve it, and train your LDA model again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dbe9de-c9d9-41a1-bf5a-10220ce6fa38",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "967136f3-5746-4568-8d2d-d48c4d3be36c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       \n",
      "--------      --------      --------      --------      \n",
      "people        file          god           game          \n",
      "say           program       people        team          \n",
      "know          use           think         year          \n",
      "gun           window        know          play          \n",
      "armenian      edu           believe       good          \n",
      "time          server        jesus         player        \n",
      "come          image         say           win           \n",
      "kill          entry         thing         season        \n",
      "think         widget        question      think         \n",
      "armenians     display       time          hockey        \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components = 4,\n",
    "                                    learning_method=\"batch\", \n",
    "                                    max_iter=10, \n",
    "                                    random_state=42,\n",
    "                                    n_jobs=-1)\n",
    "\n",
    "# fit topic modelling transformer to pre-processed count vectorized text data\n",
    "final_news_topics = lda.fit_transform(X)\n",
    "\n",
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vec.get_feature_names_out()) \n",
    "    \n",
    "mglearn.tools.print_topics(\n",
    "    topics=range(4),\n",
    "    feature_names=feature_names,\n",
    "    sorting=sorting,\n",
    "    topics_per_chunk=10,\n",
    "    n_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fdd240-3ba0-4a9b-94f0-a88b46e00f1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Interpretation of clustering with suggested labels\n",
    "\n",
    "Above, I believe I have captured the segmentation I aimed to capture via my preprocessing steps. Here, topic 0 represents politics, topic 1: technology, topic 2: religion, and topic 3: sports. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4cbbb9-1057-43ef-ac63-842e3e4cb43c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d53117b-897c-4eba-b1bd-dfedcb3d35ff",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.5 Exploring document topic association\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Show the document topic assignment of the first five documents from `df`. \n",
    "2. Comment on the document topic assignment of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12934a67-c48d-4c42-a856-248615c6202b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_5\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1f09783a-5644-46ac-bb2f-d60947a13d21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>politics</th>\n",
       "      <th>technology</th>\n",
       "      <th>religion</th>\n",
       "      <th>sports</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You know, I was reading 18 U.S.C. 922 and some...</td>\n",
       "      <td>0.284275</td>\n",
       "      <td>0.005058</td>\n",
       "      <td>0.661248</td>\n",
       "      <td>0.049420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\nIt's not a bad question: I don't have an...</td>\n",
       "      <td>0.399606</td>\n",
       "      <td>0.137476</td>\n",
       "      <td>0.459120</td>\n",
       "      <td>0.003799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nActuallay I don't, but on the other hand I d...</td>\n",
       "      <td>0.244973</td>\n",
       "      <td>0.204181</td>\n",
       "      <td>0.546133</td>\n",
       "      <td>0.004713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The following problem is really bugging me,\\na...</td>\n",
       "      <td>0.009497</td>\n",
       "      <td>0.971745</td>\n",
       "      <td>0.009950</td>\n",
       "      <td>0.008807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n  This is the latest from UPI \\n\\n     For...</td>\n",
       "      <td>0.981922</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.006180</td>\n",
       "      <td>0.005945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  politics  technology  \\\n",
       "0  You know, I was reading 18 U.S.C. 922 and some...  0.284275    0.005058   \n",
       "1  \\n\\n\\nIt's not a bad question: I don't have an...  0.399606    0.137476   \n",
       "2  \\nActuallay I don't, but on the other hand I d...  0.244973    0.204181   \n",
       "3  The following problem is really bugging me,\\na...  0.009497    0.971745   \n",
       "4  \\n\\n  This is the latest from UPI \\n\\n     For...  0.981922    0.005952   \n",
       "\n",
       "   religion    sports  \n",
       "0  0.661248  0.049420  \n",
       "1  0.459120  0.003799  \n",
       "2  0.546133  0.004713  \n",
       "3  0.009950  0.008807  \n",
       "4  0.006180  0.005945  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assignments = {'document': [],\n",
    "               'politics': [],\n",
    "               'technology': [],\n",
    "               'religion': [],\n",
    "               'sports': []}\n",
    "\n",
    "list_topics = ['politics', 'technology', 'religion', 'sports']\n",
    "\n",
    "for i in range(5):\n",
    "    assignments['document'].append(df.iloc[i]['text'])\n",
    "\n",
    "    for n in range(4):\n",
    "        assignments[list_topics[n]].append(final_news_topics[i][n])\n",
    "\n",
    "pd.DataFrame(assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "4638657b-352f-4cd0-b5c6-4d9c845b1cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0:\n",
      "You know, I was reading 18 U.S.C. 922 and something just did not make \n",
      "sence and I was wondering if someone could help me out.\n",
      "\n",
      "Say U.S.C. 922 :\n",
      "\n",
      "(1) Except as provided in paragraph (2), it shall be unlawful for\n",
      "any person to transfer or possess a machinegun.\n",
      "\n",
      " Well I got to looking in my law dictionary and I found that a \"person\" \n",
      "might also be an artificial entity that is created by government \n",
      "and has no rights under the federal constitution. So, what I \n",
      "don't understand is how a statute like 922 can be enforced on \n",
      "an individual. So someone tell me how my government can tell\n",
      "me what I can or cannot possess. Just passing a law \n",
      "does not make it LAW. Everyone knows that laws are constitional\n",
      "until it goes to court. So, has it ever gone to court, not\n",
      "just your run of the mill \"Ok I had it I am guilty, put me in jail\"\n",
      "\n",
      "Has anyone ever claimed that they had a right to possess and was told\n",
      "by the Supreme Court that they didn't have that right?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Document 1:\n",
      "\n",
      "\n",
      "\n",
      "It's not a bad question: I don't have any refs that list this algorithm\n",
      "either. But thinking about it a bit, it shouldn't be too hard.\n",
      "\n",
      "1) Take three of the points and find the plane they define as well as\n",
      "the circle that they lie on (you say you have this algorithm already)\n",
      "\n",
      "2) Find the center  of this circle. The line passing through this center\n",
      "perpendicular to the plane of the three points passes through the center of\n",
      "the sphere.\n",
      "\n",
      "3) Repeat with the unused point and two of the original points. This\n",
      "gives you two different lines that both pass through the sphere's\n",
      "origin. Their interection is the center of the sphere.\n",
      "\n",
      "4) the radius is easy to compute, it's just the distance from the center to\n",
      "any of the original points.\n",
      "\n",
      "I'll leave the math to you, but this is a workable algorithm. :-)\n",
      "\n",
      "\n",
      "An alternate method would be to take pairs of points: the plane formed\n",
      "by the perpendicular bisector of each line segment pair also contains the\n",
      "center of the sphere. Three pairs will form three planes, intersecting\n",
      "at a point. This might be easier to implement.\n",
      "\n",
      "Document 2:\n",
      "\n",
      "Actuallay I don't, but on the other hand I don't support the idea of having\n",
      "one newsgroup for every aspect of graphics programming as proposed by Brian,\n",
      "in his reply to my original posting.\n",
      "I would suggest a looser structure more like a comp.graphics.programmer,\n",
      "comp.graphics.hw_and_sw\n",
      "The reason for making as few groups as possible is for the same reason you\n",
      "say we shouldn't spilt up, not to get to few postings every day.\n",
      "I takes to much time to browse through all postings just to find two or \n",
      "three I'm interested in.\n",
      "\n",
      "I understand and agree when you say you want all aspects of graphics in one\n",
      "meeting. I agree to some extension. I see news as a forum to exchange ideas,\n",
      "help others or to be helped. I think this is difficult to achive if there\n",
      "are so many different things in one meeting.\n",
      "\n",
      "Good evening netters|-)\n",
      "\n",
      "Document 3:\n",
      "The following problem is really bugging me,\n",
      "and I would appreciate any help.\n",
      "\n",
      "I create two windows:\n",
      "\n",
      "w1 (child to root) with event_mask = ButtonPressMask|KeyPressMask;\n",
      "w2 (child to w1) with do_not_propagate_mask = ButtonPressMask|KeyPressMask;\n",
      "\n",
      "\n",
      "Keypress events in w2 are discarded, but ButtonPress events fall through\n",
      "to w1, with subwindow set to w2.\n",
      "\n",
      "FYI, I'm using xnews/olvwm.\n",
      "\n",
      "Am I doing something fundamentally wrong here?\n",
      "\n",
      "Document 4:\n",
      "\n",
      "\n",
      "  This is the latest from UPI \n",
      "\n",
      "     Foreign Ministry spokesman Ferhat Ataman told journalists Turkey was\n",
      "     closing its air space to all flights to and from Armenia and would\n",
      "     prevent humanitarian aid from reaching the republic overland across\n",
      "     Turkish territory.\n",
      "\n",
      "  \n",
      "   Historically even the most uncivilized of peoples have exhibited \n",
      "   signs of compassion by allowing humanitarian aid to reach civilian\n",
      "   populations. Even the Nazis did this much.\n",
      "\n",
      "   It seems as though from now on Turkey will publicly pronounce \n",
      "   themselves 'hypocrites' should they choose to continue their\n",
      "   condemnation of the Serbians.\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Document {i}:\\n{df.iloc[i]['text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb21d4d-862d-435d-9eff-f352c9b0e3ac",
   "metadata": {},
   "source": [
    "#### Document topic assignments commentary\n",
    "\n",
    "The first document (about gun laws and the right to possess) seems to have been classified as an intersection between politics and religion, but is arguably more about politics than it is religion. The second document, a mathematical algorithm, seems like it belongs in the technology classification, but has been horribly misclassified in politics and religion again. I am uncertain about the contents of document three, but I can tell it belongs in the technology category while it was once again misclassified. The fourth document, clearly about technology, was rightfully classified as so with 97% of its contents pertaining to the 'technology' cluster. The final document, clearly about war and politics, was rightfully classified with 98% of its contents pertaining to the 'politics' category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e21cac-5f1a-4480-b683-47c3b41a1199",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42685c5-01cb-4495-aae6-f803d6f75172",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Exercise 3: Short answer questions \n",
    "<hr>\n",
    "\n",
    "rubric={points}\n",
    "\n",
    "1. Briefly explain how content-based filtering works in the context of recommender systems. \n",
    "2. Discuss at least two negative consequences of recommender systems.\n",
    "3. What is transfer learning in natural language processing? Briefly explain.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01807641-531e-454a-9374-fc8ad04bc30b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f9fa2",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12929238-7f2c-421e-bb9a-32f0debf0636",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d6d11-0ef5-4315-9854-f64a427e62af",
   "metadata": {},
   "source": [
    "**Before submitting your assignment, please make sure you have followed all the instructions in the Submission instructions section at the top.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491f1d3-dd84-4ed2-b2b3-9148ac8df7a5",
   "metadata": {},
   "source": [
    "![](img/eva-well-done.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330] *",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
